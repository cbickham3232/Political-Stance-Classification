{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209b228-dfee-436d-b26e-83dcd4c354de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers pandas datasets peft bitsandbytes huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999e929-94df-4498-88ce-e536f7e44574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Authentication\n",
    "login(token=\"HF_TOKEN\")\n",
    "\n",
    "# Configuration\n",
    "batch_size = 8\n",
    "input_csv_path = './election_data.csv'\n",
    "output_csv_path = './mistral_predictions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a5d71-042b-48c3-8a11-e800945eef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load base model and tokenizer\n",
    "print(\"Loading model and tokenizer...\")\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Fix padding token\n",
    "\n",
    "# Load fine-tuned adapter\n",
    "print(\"Loading adapter...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"Dev-ZC/mistral-leaning-model-zaki\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed00880c-b7c9-4893-9e59-8c3747bc44d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "classification_prompt = \"\"\"You are a political statement classifier. For each political statement, respond with exactly one label: 'Left-Leaning', 'Right-Leaning', or 'Neutral'.\n",
    "Statement: \"The government must ensure healthcare is a right for all citizens. Go blue vote blue. We support Kamala Harris and we DO NOT like Donald Trump\"\n",
    "Classification: Left-Leaning\n",
    "Statement: \"We need to secure the border and stop illegal immigration. Go red vote red. We support Donald Trump and we DO NOT like Kamala and Biden\"\n",
    "Classification: Right-Leaning\n",
    "Statement: \"Pizza is the best comfort food.\"\n",
    "Classification: Neutral\n",
    "Statement: {text}\n",
    "Classify the following political statement as exactly one word - either 'Left-Leaning', 'Right-Leaning', or 'Neutral'. Only output that single classification word and nothing else.\"\"\"\n",
    "\n",
    "classification_prompt_simple = \"\"\"You are a political expert knowledgeable in U.S. politics. Your task is to classify the following social media video transcripts as leaning towards 'Left-Leaning', 'Right-Leaning', or 'Neutral' based on their content.\n",
    "A Right-leaning transcript is one that strongly criticizes Left-leaning policies or figures, expresses support for conservative values, or aligns with Left-leaning political narratives.\n",
    "A Left-leaning transcript is one that strongly criticizes Right-leaning policies or figures, expresses support for progressive values, or aligns with Right-Leaning political narratives.\n",
    "A Neutral transcript is one that does not explicitly align with either party, such as general news reports, discussions on global issues, or non-partisan commentary.\n",
    "\"\"\"\n",
    "\n",
    "# Create output file with headers if it doesn't exist\n",
    "if not os.path.exists(output_csv_path):\n",
    "    with open(output_csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['index', 'leaning'])\n",
    "\n",
    "# Load already processed indices\n",
    "# Load the full dataset\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "processed_indices = set()\n",
    "if os.path.exists(output_csv_path):\n",
    "    with open(output_csv_path, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            # Handle both integer and tensor-formatted indices\n",
    "            index_str = row['index']\n",
    "            if 'tensor' in index_str:\n",
    "                # Extract the number from tensor(129624)\n",
    "                index_value = int(index_str.split('(')[1].split(')')[0])\n",
    "            else:\n",
    "                index_value = int(index_str)\n",
    "            processed_indices.add(index_value)\n",
    "    print(f\"Already processed: {len(processed_indices)} rows\")\n",
    "\n",
    "# Load and prepare data ---<\n",
    "# Choose which partition to process (0-4)\n",
    "TOTAL_PARTITIONS = 5\n",
    "CURRENT_PARTITION = 2  # Change this to 0, 1, 2, 3, or 4 for each instance\n",
    "\n",
    "# Split the data\n",
    "total_rows = len(df)\n",
    "rows_per_partition = total_rows // TOTAL_PARTITIONS\n",
    "start_idx = CURRENT_PARTITION * rows_per_partition\n",
    "end_idx = start_idx + rows_per_partition if CURRENT_PARTITION < TOTAL_PARTITIONS - 1 else total_rows\n",
    "\n",
    "# Get just this partition\n",
    "df_partition = df.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "# Then filter for unprocessed rows within this partition\n",
    "df_unprocessed = df_partition[~df_partition.index.isin(processed_indices)].reset_index()\n",
    "\n",
    "print(f\"Processing partition {CURRENT_PARTITION+1}/{TOTAL_PARTITIONS}\")\n",
    "print(f\"Rows {start_idx} to {end_idx-1}\")\n",
    "print(f\"Processing {len(df_unprocessed)} unprocessed rows out of {len(df_partition)} in this partition\")\n",
    "# ---<\n",
    "\n",
    "# Create dataset\n",
    "def preprocess_function(examples):\n",
    "    texts = examples[\"whisper_voice_to_text\"]\n",
    "    prompts = [f\"<s>[INST] {classification_prompt.format(text=text)} [/INST]\" for text in texts]\n",
    "    return {\"prompt\": prompts, \"original_index\": examples[\"index\"]}\n",
    "\n",
    "# Convert DataFrame to Dataset\n",
    "dataset = Dataset.from_pandas(df_unprocessed)\n",
    "dataset = dataset.map(preprocess_function, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"prompt\", \"original_index\"])\n",
    "\n",
    "# Process in batches and save incrementally\n",
    "def classify_and_save_batch(batch):\n",
    "    prompts = batch[\"prompt\"]\n",
    "    indices = batch[\"original_index\"]\n",
    "    \n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=15,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    results = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    \n",
    "    # Extract classifications\n",
    "    classifications = []\n",
    "    for prompt, result in zip(prompts, results):\n",
    "        prompt_text = prompt.split(\"[/INST]\")[0] + \"[/INST]\"\n",
    "        classification = result[len(prompt_text):].strip()\n",
    "        \n",
    "        # Extract the label\n",
    "        if \"Left-Leaning\" in classification:\n",
    "            cleaned = \"Left-Leaning\"\n",
    "        elif \"Right-Leaning\" in classification:\n",
    "            cleaned = \"Right-Leaning\"\n",
    "        elif \"Neutral\" in classification:\n",
    "            cleaned = \"Neutral\"\n",
    "        else:\n",
    "            cleaned = classification[:20]  # Fallback\n",
    "            \n",
    "        classifications.append(cleaned)\n",
    "    \n",
    "    # Save this batch results to CSV\n",
    "    with open(output_csv_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for idx, classification in zip(indices, classifications):\n",
    "            writer.writerow([idx, classification])\n",
    "    \n",
    "    return classifications\n",
    "\n",
    "# Process with datasets dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Run inference\n",
    "print(\"Running inference with incremental saving...\")\n",
    "start_time = time.time()\n",
    "all_results = []\n",
    "\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    batch_results = classify_and_save_batch(batch)\n",
    "    all_results.extend(batch_results)\n",
    "    \n",
    "    # Print progress update\n",
    "    if (i+1) % 5 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Batch {i+1}/{len(dataloader)}: {len(all_results)} samples processed, {elapsed:.2f}s total ({elapsed/len(all_results):.2f}s/sample)\")\n",
    "\n",
    "# Final report\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nâœ… Done! Total time: {elapsed:.2f} seconds ({elapsed/len(all_results):.2f}s/sample)\")\n",
    "print(f\"Results saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ebfe81-cb44-4093-b289-d59d5b6c9144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
